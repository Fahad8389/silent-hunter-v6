{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silent Hunter v6.0 - ISS Metagenome Novel Protein Discovery\n",
    "\n",
    "**100% Data Publication Pipeline**\n",
    "\n",
    "This notebook processes the complete ISS metagenome dataset (GLDS-69, SRR6356483) to discover novel proteins with publication-ready methodology.\n",
    "\n",
    "## Requirements\n",
    "- Google Colab Pro (recommended for 100GB+ disk space)\n",
    "- ~7-10 hours total runtime\n",
    "- Google Drive for persistent storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 0: Setup and Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and setup logging\n",
    "from google.colab import drive\n",
    "import datetime\n",
    "import hashlib\n",
    "import os\n",
    "import json\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directory structure\n",
    "BASE_DIR = '/content/drive/MyDrive/SilentHunter_v6'\n",
    "dirs = ['data', 'intermediate', 'databases', 'output', 'audit']\n",
    "for d in dirs:\n",
    "    os.makedirs(f'{BASE_DIR}/{d}', exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "SRA_ACCESSION = 'SRR6356483'\n",
    "THREADS = 4\n",
    "\n",
    "# Audit logging\n",
    "AUDIT_LOG = f'{BASE_DIR}/audit/commands.log'\n",
    "\n",
    "def log_command(cmd, output=\"\"):\n",
    "    with open(AUDIT_LOG, 'a') as f:\n",
    "        f.write(f\"\\n{'='*60}\\n\")\n",
    "        f.write(f\"TIME: {datetime.datetime.now().isoformat()}\\n\")\n",
    "        f.write(f\"CMD: {cmd}\\n\")\n",
    "        if output:\n",
    "            f.write(f\"OUTPUT: {output[:500]}...\\n\")\n",
    "    print(f\"[LOGGED] {cmd[:50]}...\")\n",
    "\n",
    "log_command(\"Session started\")\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(\"Audit logging enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SRA Toolkit\n",
    "!pip install -q sra-tools\n",
    "\n",
    "# Download 100% data (NO downsampling)\n",
    "cmd = f\"fastq-dump --split-files --gzip {SRA_ACCESSION}\"\n",
    "!{cmd}\n",
    "log_command(cmd)\n",
    "\n",
    "# Verify download\n",
    "!ls -lh {SRA_ACCESSION}*.fastq.gz\n",
    "\n",
    "# Count reads\n",
    "!echo \"Read count:\" && zcat {SRA_ACCESSION}_1.fastq.gz | wc -l | awk '{{print $1/4 \" reads\"}}'\n",
    "\n",
    "# Calculate checksums\n",
    "!md5sum {SRA_ACCESSION}*.fastq.gz > {BASE_DIR}/audit/raw_data.md5\n",
    "!cat {BASE_DIR}/audit/raw_data.md5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Quality Control (fastp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install fastp\n",
    "!apt-get install -qq fastp\n",
    "\n",
    "# Run fastp\n",
    "cmd = f\"\"\"fastp \\\n",
    "    -i {SRA_ACCESSION}_1.fastq.gz \\\n",
    "    -I {SRA_ACCESSION}_2.fastq.gz \\\n",
    "    -o clean_1.fastq.gz \\\n",
    "    -O clean_2.fastq.gz \\\n",
    "    --detect_adapter_for_pe \\\n",
    "    --cut_front --cut_tail \\\n",
    "    --cut_window_size 4 \\\n",
    "    --cut_mean_quality 20 \\\n",
    "    --length_required 50 \\\n",
    "    --html {BASE_DIR}/audit/fastp_report.html \\\n",
    "    --json {BASE_DIR}/audit/fastp_report.json \\\n",
    "    --thread {THREADS}\"\"\"\n",
    "\n",
    "!{cmd}\n",
    "log_command(cmd)\n",
    "\n",
    "# Display QC stats\n",
    "with open(f'{BASE_DIR}/audit/fastp_report.json') as f:\n",
    "    data = json.load(f)\n",
    "    before = data['summary']['before_filtering']\n",
    "    after = data['summary']['after_filtering']\n",
    "    print(f\"\\nQC Statistics:\")\n",
    "    print(f\"Reads before: {before['total_reads']:,}\")\n",
    "    print(f\"Reads after: {after['total_reads']:,}\")\n",
    "    print(f\"Survival rate: {after['total_reads']/before['total_reads']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Assembly (MEGAHIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MEGAHIT\n",
    "!wget -q https://github.com/voutcn/megahit/releases/download/v1.2.9/MEGAHIT-1.2.9-Linux-x86_64-static.tar.gz\n",
    "!tar -xzf MEGAHIT-1.2.9-Linux-x86_64-static.tar.gz\n",
    "!cp MEGAHIT-1.2.9-Linux-x86_64-static/bin/megahit* /usr/local/bin/\n",
    "\n",
    "# Run MEGAHIT\n",
    "cmd = f\"\"\"megahit \\\n",
    "    -1 clean_1.fastq.gz \\\n",
    "    -2 clean_2.fastq.gz \\\n",
    "    -o assembly \\\n",
    "    --min-contig-len 500 \\\n",
    "    --k-min 21 \\\n",
    "    --k-max 141 \\\n",
    "    --k-step 12 \\\n",
    "    -t {THREADS} \\\n",
    "    -m 0.9\"\"\"\n",
    "\n",
    "!{cmd}\n",
    "log_command(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate assembly statistics\n",
    "seqs = []\n",
    "with open('assembly/final.contigs.fa') as f:\n",
    "    s = ''\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            if s: seqs.append(len(s))\n",
    "            s = ''\n",
    "        else: s += line.strip()\n",
    "    if s: seqs.append(len(s))\n",
    "\n",
    "seqs.sort(reverse=True)\n",
    "total = sum(seqs)\n",
    "\n",
    "# N50\n",
    "n50_sum = 0\n",
    "for l in seqs:\n",
    "    n50_sum += l\n",
    "    if n50_sum >= total/2:\n",
    "        print(f'N50: {l:,} bp')\n",
    "        break\n",
    "\n",
    "print(f'Total contigs: {len(seqs):,}')\n",
    "print(f'Total assembly size: {total:,} bp')\n",
    "print(f'Largest contig: {seqs[0]:,} bp')\n",
    "\n",
    "# Save to Drive\n",
    "!cp -r assembly {BASE_DIR}/intermediate/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: ORF Prediction (Prodigal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Prodigal\n",
    "!apt-get install -qq prodigal\n",
    "\n",
    "# Run Prodigal in metagenomic mode\n",
    "cmd = \"\"\"prodigal \\\n",
    "    -i assembly/final.contigs.fa \\\n",
    "    -a proteins.faa \\\n",
    "    -d genes.fna \\\n",
    "    -o genes.gff \\\n",
    "    -f gff \\\n",
    "    -p meta \\\n",
    "    -q\"\"\"\n",
    "\n",
    "!{cmd}\n",
    "log_command(cmd)\n",
    "\n",
    "# Count ORFs\n",
    "!echo \"Total proteins:\" && grep -c \">\" proteins.faa\n",
    "\n",
    "# Save to Drive\n",
    "!mkdir -p {BASE_DIR}/intermediate/orfs\n",
    "!cp proteins.faa genes.fna genes.gff {BASE_DIR}/intermediate/orfs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5A: Copy/Build UniRef90 Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install DIAMOND\n",
    "!wget -q https://github.com/bbuchfink/diamond/releases/download/v2.1.8/diamond-linux64.tar.gz\n",
    "!tar -xzf diamond-linux64.tar.gz\n",
    "!mv diamond /usr/local/bin/\n",
    "\n",
    "# Check if UniRef90 database exists from previous session\n",
    "import os\n",
    "UNIREF90_DB = f'{BASE_DIR}/databases/uniref90.dmnd'\n",
    "\n",
    "if os.path.exists(UNIREF90_DB):\n",
    "    print(f\"Found existing UniRef90 database: {UNIREF90_DB}\")\n",
    "    !cp {UNIREF90_DB} .\n",
    "else:\n",
    "    print(\"UniRef90 database not found. Downloading (this will take ~40 minutes)...\")\n",
    "    !wget -q --show-progress https://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref90/uniref90.fasta.gz\n",
    "    !diamond makedb --in uniref90.fasta.gz -d uniref90 --threads {THREADS}\n",
    "    !cp uniref90.dmnd {BASE_DIR}/databases/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5B: DIAMOND Search vs UniRef90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAMOND search against UniRef90\n",
    "cmd = \"\"\"diamond blastp \\\n",
    "    -q proteins.faa \\\n",
    "    -d uniref90 \\\n",
    "    -o uniref90_hits.m8 \\\n",
    "    --id 25 \\\n",
    "    --evalue 1e-5 \\\n",
    "    --sensitive \\\n",
    "    --threads 4 \\\n",
    "    --max-target-seqs 1\"\"\"\n",
    "\n",
    "!{cmd}\n",
    "log_command(cmd)\n",
    "\n",
    "!echo \"Proteins with UniRef90 hits:\" && cut -f1 uniref90_hits.m8 | sort -u | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5C: SwissProt Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and build SwissProt database\n",
    "!wget -q https://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/swissprot.gz\n",
    "!gunzip -f swissprot.gz\n",
    "!diamond makedb --in swissprot -d swissprot --quiet\n",
    "\n",
    "# Search\n",
    "cmd = \"\"\"diamond blastp \\\n",
    "    -q proteins.faa \\\n",
    "    -d swissprot \\\n",
    "    -o swissprot_hits.m8 \\\n",
    "    --id 25 \\\n",
    "    --evalue 1e-5 \\\n",
    "    --sensitive \\\n",
    "    --threads 4\"\"\"\n",
    "\n",
    "!{cmd}\n",
    "log_command(cmd)\n",
    "\n",
    "!echo \"Proteins with SwissProt hits:\" && cut -f1 swissprot_hits.m8 | sort -u | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5D: Human Contamination Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download human proteome\n",
    "!wget -q https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/reference_proteomes/Eukaryota/UP000005640/UP000005640_9606.fasta.gz\n",
    "!gunzip -f UP000005640_9606.fasta.gz\n",
    "!diamond makedb --in UP000005640_9606.fasta -d human --quiet\n",
    "\n",
    "# Search with relaxed thresholds\n",
    "cmd = \"\"\"diamond blastp \\\n",
    "    -q proteins.faa \\\n",
    "    -d human \\\n",
    "    -o human_hits.m8 \\\n",
    "    --id 50 \\\n",
    "    --evalue 1e-10 \\\n",
    "    --threads 4\"\"\"\n",
    "\n",
    "!{cmd}\n",
    "log_command(cmd)\n",
    "\n",
    "human_count = !cut -f1 human_hits.m8 | sort -u | wc -l\n",
    "print(f\"Human contamination: {human_count[0]} proteins\")\n",
    "if int(human_count[0]) > 0:\n",
    "    print(\"WARNING: Human contamination detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5E: Chimera Detection (VSEARCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install VSEARCH\n",
    "!wget -q https://github.com/torognes/vsearch/releases/download/v2.22.1/vsearch-2.22.1-linux-x86_64.tar.gz\n",
    "!tar -xzf vsearch-2.22.1-linux-x86_64.tar.gz\n",
    "!cp vsearch-2.22.1-linux-x86_64/bin/vsearch /usr/local/bin/\n",
    "\n",
    "# De novo chimera detection\n",
    "cmd = \"\"\"vsearch \\\n",
    "    --uchime_denovo genes.fna \\\n",
    "    --chimeras chimeras.fna \\\n",
    "    --nonchimeras clean_genes.fna \\\n",
    "    --quiet\"\"\"\n",
    "\n",
    "!{cmd}\n",
    "log_command(cmd)\n",
    "\n",
    "!echo \"Potential chimeras detected:\" && grep -c \">\" chimeras.fna || echo \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Extract Novel Proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all protein IDs\n",
    "all_proteins = set()\n",
    "with open('proteins.faa') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            all_proteins.add(line[1:].split()[0])\n",
    "\n",
    "# Get proteins WITH hits\n",
    "def get_hits(filename):\n",
    "    hits = set()\n",
    "    try:\n",
    "        with open(filename) as f:\n",
    "            for line in f:\n",
    "                hits.add(line.split()[0])\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    return hits\n",
    "\n",
    "uniref90_hits = get_hits('uniref90_hits.m8')\n",
    "swissprot_hits = get_hits('swissprot_hits.m8')\n",
    "human_hits = get_hits('human_hits.m8')\n",
    "\n",
    "# Novel = no hits anywhere\n",
    "all_hits = uniref90_hits | swissprot_hits | human_hits\n",
    "novel = all_proteins - all_hits\n",
    "\n",
    "print(f\"Total proteins: {len(all_proteins):,}\")\n",
    "print(f\"UniRef90 hits: {len(uniref90_hits):,}\")\n",
    "print(f\"SwissProt hits: {len(swissprot_hits):,}\")\n",
    "print(f\"Human hits: {len(human_hits):,}\")\n",
    "print(f\"NOVEL (no hits): {len(novel):,}\")\n",
    "\n",
    "# Extract novel sequences\n",
    "seqs = {}\n",
    "with open('proteins.faa') as f:\n",
    "    h = None\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            h = line[1:].split()[0]\n",
    "            seqs[h] = line\n",
    "        else:\n",
    "            seqs[h] += line\n",
    "\n",
    "with open('novel_raw.faa', 'w') as f:\n",
    "    for pid in novel:\n",
    "        f.write(seqs[pid])\n",
    "\n",
    "log_command(f\"Extracted {len(novel)} novel proteins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Quality Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load novel proteins\n",
    "novels = []\n",
    "with open('novel_raw.faa') as f:\n",
    "    h, s = None, ''\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            if h: novels.append((h, s))\n",
    "            h, s = line[1:].strip(), ''\n",
    "        else:\n",
    "            s += line.strip()\n",
    "    if h: novels.append((h, s))\n",
    "\n",
    "# Apply filters\n",
    "filtered = []\n",
    "rejected = {'short': 0, 'no_start': 0, 'internal_stop': 0, 'partial': 0}\n",
    "\n",
    "for header, seq in novels:\n",
    "    if len(seq) < 100:\n",
    "        rejected['short'] += 1\n",
    "    elif not seq.startswith('M'):\n",
    "        rejected['no_start'] += 1\n",
    "    elif '*' in seq[:-1]:\n",
    "        rejected['internal_stop'] += 1\n",
    "    elif 'partial=10' in header or 'partial=01' in header or 'partial=11' in header:\n",
    "        rejected['partial'] += 1\n",
    "    else:\n",
    "        filtered.append((header, seq))\n",
    "\n",
    "print(f\"\\nQUALITY FILTERING\")\n",
    "print(f\"Input: {len(novels):,}\")\n",
    "print(f\"Passed: {len(filtered):,}\")\n",
    "print(f\"\\nRejected:\")\n",
    "for reason, count in sorted(rejected.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {reason}: {count:,}\")\n",
    "\n",
    "# Save filtered\n",
    "with open(f'{BASE_DIR}/output/truly_novel.faa', 'w') as f:\n",
    "    for h, s in filtered:\n",
    "        f.write(f'>{h}\\n{s}\\n')\n",
    "\n",
    "log_command(f\"Quality filtered: {len(novels)} -> {len(filtered)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8A: Amino Acid Composition Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "EXPECTED = {'A': 0.083, 'R': 0.055, 'N': 0.041, 'D': 0.055, 'C': 0.014,\n",
    "            'Q': 0.039, 'E': 0.068, 'G': 0.071, 'H': 0.023, 'I': 0.060,\n",
    "            'L': 0.097, 'K': 0.058, 'M': 0.024, 'F': 0.039, 'P': 0.047,\n",
    "            'S': 0.066, 'T': 0.053, 'W': 0.011, 'Y': 0.029, 'V': 0.069}\n",
    "\n",
    "all_aa = ''.join(s for h, s in filtered)\n",
    "counts = Counter(all_aa)\n",
    "total = sum(counts[aa] for aa in EXPECTED.keys() if aa in counts)\n",
    "\n",
    "chi_sq = 0\n",
    "for aa, exp in EXPECTED.items():\n",
    "    obs = counts.get(aa, 0) / total if total > 0 else 0\n",
    "    chi_sq += ((obs - exp) ** 2) / exp\n",
    "\n",
    "print(f\"Chi-squared: {chi_sq:.3f}\")\n",
    "print(\"< 0.3 = normal protein composition\")\n",
    "print(\"> 0.5 = suspicious\")\n",
    "verdict_aa = \"PASS\" if chi_sq < 0.3 else \"FAIL\"\n",
    "print(f\"VERDICT: {verdict_aa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8B: Protein Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(seq):\n",
    "    issues = []\n",
    "    for aa in 'AGLPQRS':\n",
    "        if aa * 5 in seq: issues.append('repetitive')\n",
    "    if sum(1 for a in seq if a in 'AILMFWV') / len(seq) > 0.6:\n",
    "        issues.append('too_hydrophobic')\n",
    "    if sum(1 for a in seq if a in 'DEKR') / len(seq) > 0.4:\n",
    "        issues.append('too_charged')\n",
    "    return 'likely_real' if len(issues) == 0 else 'possibly_artifact'\n",
    "\n",
    "real, artifact = 0, 0\n",
    "for h, s in filtered:\n",
    "    if classify(s) == 'likely_real':\n",
    "        real += 1\n",
    "    else:\n",
    "        artifact += 1\n",
    "\n",
    "print(f\"Likely real: {real} ({real/len(filtered)*100:.1f}%)\")\n",
    "print(f\"Possibly artifact: {artifact}\")\n",
    "verdict_class = \"PASS\" if real/len(filtered) > 0.9 else \"REVIEW\"\n",
    "print(f\"VERDICT: {verdict_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8C: Genomic Context Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Parse GFF to get ORF positions\n",
    "contig_orfs = defaultdict(list)\n",
    "with open('genes.gff') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('#') or not line.strip():\n",
    "            continue\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) >= 9 and parts[2] == 'CDS':\n",
    "            contig = parts[0]\n",
    "            start, end = int(parts[3]), int(parts[4])\n",
    "            pid_match = re.search(r'ID=([^;]+)', parts[8])\n",
    "            if pid_match:\n",
    "                pid = pid_match.group(1)\n",
    "                contig_orfs[contig].append((start, end, pid))\n",
    "\n",
    "# Find isolated ORFs\n",
    "novel_ids = set(h.split()[0] for h, s in filtered)\n",
    "isolated_novels = []\n",
    "\n",
    "for contig, orfs in contig_orfs.items():\n",
    "    for start, end, pid in orfs:\n",
    "        if pid in novel_ids:\n",
    "            if len(orfs) == 1:\n",
    "                isolated_novels.append((pid, 'only_orf_on_contig'))\n",
    "            elif max(e for s, e, p in orfs) - min(s for s, e, p in orfs) < 500:\n",
    "                isolated_novels.append((pid, 'very_short_contig'))\n",
    "\n",
    "print(f\"Isolated novel ORFs (lower confidence): {len(isolated_novels)}\")\n",
    "print(f\"Contextual novel ORFs (higher confidence): {len(novel_ids) - len(isolated_novels)}\")\n",
    "verdict_context = \"REVIEW\" if len(isolated_novels) > len(novel_ids) * 0.3 else \"PASS\"\n",
    "print(f\"VERDICT: {verdict_context}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8D: HHblits Sample Generation (Manual Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "sample_20 = random.sample(filtered, min(20, len(filtered)))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"HHblits REMOTE HOMOLOGY TEST (Manual Step)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nInstructions:\")\n",
    "print(\"1. Go to: https://toolkit.tuebingen.mpg.de/tools/hhpred\")\n",
    "print(\"2. Select database: PDB_mmCIF70, UniRef30\")\n",
    "print(\"3. Paste each sequence below, one at a time\")\n",
    "print(\"4. Record: Match found (Type C) or No match (potentially Type A)\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for i, (h, s) in enumerate(sample_20, 1):\n",
    "    print(f\"=== Sample {i}/20 ===\")\n",
    "    print(f\"ID: {h.split()[0]}\")\n",
    "    print(f\"Length: {len(s)} aa\")\n",
    "    print(f\"Sequence:\\n{s[:200]}{'...' if len(s) > 200 else ''}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8E: Foldseek Sample Generation (Manual Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_10 = random.sample(filtered, min(10, len(filtered)))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FOLDSEEK STRUCTURAL SEARCH (Manual Step)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nInstructions:\")\n",
    "print(\"1. Go to: https://search.foldseek.com/search\")\n",
    "print(\"2. For each sequence below:\")\n",
    "print(\"   a. First predict structure at: https://esmatlas.com/resources?action=fold\")\n",
    "print(\"   b. Download the PDB file\")\n",
    "print(\"   c. Upload to Foldseek, search against PDB, AlphaFold\")\n",
    "print(\"3. Record: Structure match (Type B) or No match (potentially Type A)\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for i, (h, s) in enumerate(sample_10, 1):\n",
    "    print(f\"=== Sample {i}/10 ===\")\n",
    "    print(f\"ID: {h.split()[0]}\")\n",
    "    print(f\"Length: {len(s)} aa\")\n",
    "    if len(s) > 400:\n",
    "        print(\"WARNING: >400aa, may need to truncate for ESMFold\")\n",
    "    print(f\"Sequence:\\n{s[:300]}{'...' if len(s) > 300 else ''}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8F: Generate Verification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "classifications = {\n",
    "    'type_a_completely_novel': [],\n",
    "    'type_b_structure_known': [],\n",
    "    'type_c_remote_homolog': [],\n",
    "    'type_d_domain_hybrid': [],\n",
    "    'type_e_artifact': []\n",
    "}\n",
    "\n",
    "# Add artifacts\n",
    "artifact_ids = set()\n",
    "for pid, reason in isolated_novels:\n",
    "    classifications['type_e_artifact'].append((pid, reason))\n",
    "    artifact_ids.add(pid)\n",
    "\n",
    "# Remaining as Type A (preliminary)\n",
    "for h, s in filtered:\n",
    "    pid = h.split()[0]\n",
    "    if pid not in artifact_ids:\n",
    "        classifications['type_a_completely_novel'].append(pid)\n",
    "\n",
    "# Generate report\n",
    "report = f\"\"\"# Verification Report - Silent Hunter v6.0\n",
    "\n",
    "Generated: {datetime.datetime.now().isoformat()}\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Total proteins predicted | {len(all_proteins):,} |\n",
    "| Novel (no database hits) | {len(novel):,} |\n",
    "| Quality filtered | {len(filtered):,} |\n",
    "\n",
    "## Database Searches\n",
    "\n",
    "| Database | Size | Hits | Novel |\n",
    "|----------|------|------|-------|\n",
    "| UniRef90 | 184M | {len(uniref90_hits):,} | {len(all_proteins)-len(uniref90_hits):,} |\n",
    "| SwissProt | 570K | {len(swissprot_hits):,} | - |\n",
    "| Human | 20K | {len(human_hits):,} | - |\n",
    "\n",
    "## Verification Tests\n",
    "\n",
    "| Test | Result | Verdict |\n",
    "|------|--------|--------|\n",
    "| Human contamination | {len(human_hits)} matches | {\"PASS\" if len(human_hits) == 0 else \"FAIL\"} |\n",
    "| AA composition | Chi-sq = {chi_sq:.3f} | {verdict_aa} |\n",
    "| Protein classification | {real/len(filtered)*100:.1f}% real | {verdict_class} |\n",
    "| Genomic context | {len(isolated_novels)} isolated | {verdict_context} |\n",
    "\n",
    "## Novel Protein Classification (Preliminary)\n",
    "\n",
    "| Type | Description | Count |\n",
    "|------|-------------|-------|\n",
    "| Type A | Completely novel | {len(classifications['type_a_completely_novel'])} |\n",
    "| Type B | Structure-known | TBD (run Foldseek) |\n",
    "| Type C | Remote homolog | TBD (run HHblits) |\n",
    "| Type D | Domain hybrid | TBD (run Pfam) |\n",
    "| Type E | Possible artifact | {len(classifications['type_e_artifact'])} |\n",
    "\n",
    "## Manual Verification Required\n",
    "\n",
    "- [ ] HHblits remote homology (20 samples)\n",
    "- [ ] Foldseek structural similarity (10 samples)\n",
    "- [ ] NCBI nr spot check (10 samples)\n",
    "- [ ] Pfam domain search (full dataset)\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "**{len(filtered):,} novel protein candidates** identified from ISS metagenome.\n",
    "\n",
    "After full verification, expect:\n",
    "- **Type A (Truly Novel):** ~{int(len(filtered)*0.15)}-{int(len(filtered)*0.25)} proteins\n",
    "- **Type B-D (Classifiable):** ~{int(len(filtered)*0.6)}-{int(len(filtered)*0.7)} proteins\n",
    "- **Type E (Artifacts):** ~{int(len(filtered)*0.05)}-{int(len(filtered)*0.1)} proteins\n",
    "\"\"\"\n",
    "\n",
    "with open(f'{BASE_DIR}/output/VERIFICATION_REPORT.md', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all intermediate files to Drive\n",
    "!mkdir -p {BASE_DIR}/intermediate/diamond\n",
    "!cp *_hits.m8 {BASE_DIR}/intermediate/diamond/ 2>/dev/null || true\n",
    "\n",
    "# Calculate final checksums\n",
    "!md5sum {BASE_DIR}/output/truly_novel.faa >> {BASE_DIR}/audit/checksums.md5\n",
    "\n",
    "print(f\"\\nAll files saved to: {BASE_DIR}\")\n",
    "print(\"\\nFinal output:\")\n",
    "!ls -lh {BASE_DIR}/output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Complete!\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Manual HHblits verification** - Use the samples from Step 8D\n",
    "2. **Manual Foldseek verification** - Use the samples from Step 8E\n",
    "3. **NCBI nr spot check** - Run 10 random proteins through web BLAST\n",
    "4. **Update classification** - Based on manual results\n",
    "5. **Prepare publication** - Use METHODS.md as starting point"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
