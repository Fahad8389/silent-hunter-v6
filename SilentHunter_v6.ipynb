{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silent Hunter v6.0 - ISS Metagenome Novel Protein Discovery\n",
    "\n",
    "**100% Data Publication Pipeline**\n",
    "\n",
    "## FAHDYCONX RULES\n",
    "1. **NO FABRICATION** - Every number from actual output\n",
    "2. **DOCUMENT TRANSITIONS** - Record INPUT â†’ OUTPUT for each step\n",
    "3. **READ DOCS FIRST** - Understand tools before running\n",
    "\n",
    "## Your Files (Already on Drive)\n",
    "- `SilentHunter_v6/SRR6356483` - Raw ISS data (4.5 GB)\n",
    "- `SilentHunter_v6/uniref90.fasta.gz` - Database (35 GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 0: Setup and Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and setup logging\n",
    "from google.colab import drive\n",
    "import datetime\n",
    "import hashlib\n",
    "import os\n",
    "import json\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Base directory - your files are here\n",
    "BASE_DIR = '/content/drive/MyDrive/SilentHunter_v6'\n",
    "\n",
    "# Create subdirectories\n",
    "for d in ['intermediate', 'output', 'audit']:\n",
    "    os.makedirs(f'{BASE_DIR}/{d}', exist_ok=True)\n",
    "\n",
    "# Verify your files exist\n",
    "SRA_FILE = f'{BASE_DIR}/SRR6356483'\n",
    "UNIREF90_FASTA = f'{BASE_DIR}/uniref90.fasta.gz'\n",
    "\n",
    "print(\"Checking your files...\")\n",
    "print(f\"SRA file: {os.path.exists(SRA_FILE)} - {SRA_FILE}\")\n",
    "print(f\"UniRef90: {os.path.exists(UNIREF90_FASTA)} - {UNIREF90_FASTA}\")\n",
    "\n",
    "if not os.path.exists(SRA_FILE):\n",
    "    print(\"\\nâš ï¸ ERROR: SRR6356483 not found! Upload it to SilentHunter_v6/\")\n",
    "if not os.path.exists(UNIREF90_FASTA):\n",
    "    print(\"\\nâš ï¸ ERROR: uniref90.fasta.gz not found!\")\n",
    "\n",
    "# Configuration\n",
    "SRA_ACCESSION = 'SRR6356483'\n",
    "THREADS = 4\n",
    "\n",
    "# Audit logging\n",
    "AUDIT_LOG = f'{BASE_DIR}/audit/commands.log'\n",
    "\n",
    "def log_command(cmd, output=\"\"):\n",
    "    with open(AUDIT_LOG, 'a') as f:\n",
    "        f.write(f\"\\n{'='*60}\\n\")\n",
    "        f.write(f\"TIME: {datetime.datetime.now().isoformat()}\\n\")\n",
    "        f.write(f\"CMD: {cmd}\\n\")\n",
    "        if output:\n",
    "            f.write(f\"OUTPUT: {output[:500]}...\\n\")\n",
    "    print(f\"[LOGGED] {cmd[:50]}...\")\n",
    "\n",
    "log_command(\"Session started\")\n",
    "print(f\"\\nâœ… Base directory: {BASE_DIR}\")\n",
    "print(\"âœ… Audit logging enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Convert SRA to FASTQ (Using Your Downloaded File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SRA Toolkit\n",
    "!pip install -q sra-tools\n",
    "\n",
    "# Copy SRA file to working directory (faster than reading from Drive)\n",
    "print(\"Copying SRA file to local storage...\")\n",
    "!cp \"{BASE_DIR}/SRR6356483\" .\n",
    "!ls -lh SRR6356483\n",
    "\n",
    "# Convert SRA to FASTQ\n",
    "print(\"\\nConverting SRA to FASTQ (this takes ~10-15 minutes)...\")\n",
    "cmd = \"fastq-dump --split-files --gzip SRR6356483\"\n",
    "!{cmd}\n",
    "log_command(cmd)\n",
    "\n",
    "# Verify conversion\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1 COMPLETE: SRA â†’ FASTQ\")\n",
    "print(\"=\"*60)\n",
    "!ls -lh SRR6356483*.fastq.gz\n",
    "\n",
    "# Count reads\n",
    "read_count = !zcat SRR6356483_1.fastq.gz | wc -l\n",
    "num_reads = int(read_count[0]) // 4\n",
    "print(f\"\\nTotal reads: {num_reads:,}\")\n",
    "\n",
    "# Calculate checksums\n",
    "!md5sum SRR6356483*.fastq.gz > {BASE_DIR}/audit/raw_data.md5\n",
    "\n",
    "print(f\"\\nðŸ“Š TRANSITION LOG:\")\n",
    "print(f\"INPUT:  SRR6356483 (SRA format)\")\n",
    "print(f\"OUTPUT: SRR6356483_1.fastq.gz, SRR6356483_2.fastq.gz\")\n",
    "print(f\"READS:  {num_reads:,} paired-end reads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Quality Control (fastp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install fastp\n",
    "!apt-get install -qq fastp\n",
    "\n",
    "print(\"Running fastp quality control...\")\n",
    "print(\"Parameters:\")\n",
    "print(\"  --cut_mean_quality 20: Remove bases below Q20\")\n",
    "print(\"  --length_required 50: Remove reads < 50bp\")\n",
    "print()\n",
    "\n",
    "# Run fastp\n",
    "cmd = f\"\"\"fastp \\\n",
    "    -i SRR6356483_1.fastq.gz \\\n",
    "    -I SRR6356483_2.fastq.gz \\\n",
    "    -o clean_1.fastq.gz \\\n",
    "    -O clean_2.fastq.gz \\\n",
    "    --detect_adapter_for_pe \\\n",
    "    --cut_front --cut_tail \\\n",
    "    --cut_window_size 4 \\\n",
    "    --cut_mean_quality 20 \\\n",
    "    --length_required 50 \\\n",
    "    --html {BASE_DIR}/audit/fastp_report.html \\\n",
    "    --json {BASE_DIR}/audit/fastp_report.json \\\n",
    "    --thread {THREADS}\"\"\"\n",
    "\n",
    "!{cmd}\n",
    "log_command(cmd)\n",
    "\n",
    "# Display QC stats\n",
    "with open(f'{BASE_DIR}/audit/fastp_report.json') as f:\n",
    "    data = json.load(f)\n",
    "    before = data['summary']['before_filtering']\n",
    "    after = data['summary']['after_filtering']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2 COMPLETE: Quality Control\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ“Š TRANSITION LOG:\")\n",
    "print(f\"INPUT:  {before['total_reads']:,} reads\")\n",
    "print(f\"OUTPUT: {after['total_reads']:,} reads\")\n",
    "print(f\"CHANGE: {before['total_reads'] - after['total_reads']:,} reads removed\")\n",
    "print(f\"SURVIVAL RATE: {after['total_reads']/before['total_reads']*100:.1f}%\")\n",
    "print(f\"REASON: Low quality, short reads, adapters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Assembly (MEGAHIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MEGAHIT\n",
    "!wget -q https://github.com/voutcn/megahit/releases/download/v1.2.9/MEGAHIT-1.2.9-Linux-x86_64-static.tar.gz\n",
    "!tar -xzf MEGAHIT-1.2.9-Linux-x86_64-static.tar.gz\n",
    "!cp MEGAHIT-1.2.9-Linux-x86_64-static/bin/megahit* /usr/local/bin/\n",
    "\n",
    "print(\"Running MEGAHIT assembly (this takes 1-2 hours)...\")\n",
    "print(\"Parameters:\")\n",
    "print(\"  --min-contig-len 500: Only output contigs >= 500bp\")\n",
    "print(\"  --k-min 21 --k-max 141: K-mer range for assembly\")\n",
    "print()\n",
    "\n",
    "# Run MEGAHIT\n",
    "cmd = f\"\"\"megahit \\\n",
    "    -1 clean_1.fastq.gz \\\n",
    "    -2 clean_2.fastq.gz \\\n",
    "    -o assembly \\\n",
    "    --min-contig-len 500 \\\n",
    "    --k-min 21 \\\n",
    "    --k-max 141 \\\n",
    "    --k-step 12 \\\n",
    "    -t {THREADS} \\\n",
    "    -m 0.9\"\"\"\n",
    "\n",
    "!{cmd}\n",
    "log_command(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate assembly statistics\n",
    "seqs = []\n",
    "with open('assembly/final.contigs.fa') as f:\n",
    "    s = ''\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            if s: seqs.append(len(s))\n",
    "            s = ''\n",
    "        else: s += line.strip()\n",
    "    if s: seqs.append(len(s))\n",
    "\n",
    "seqs.sort(reverse=True)\n",
    "total = sum(seqs)\n",
    "\n",
    "# N50\n",
    "n50_sum = 0\n",
    "n50 = 0\n",
    "for l in seqs:\n",
    "    n50_sum += l\n",
    "    if n50_sum >= total/2:\n",
    "        n50 = l\n",
    "        break\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 3 COMPLETE: Assembly\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ“Š TRANSITION LOG:\")\n",
    "print(f\"INPUT:  Clean FASTQ reads\")\n",
    "print(f\"OUTPUT: {len(seqs):,} contigs\")\n",
    "print(f\"TOTAL SIZE: {total:,} bp\")\n",
    "print(f\"N50: {n50:,} bp\")\n",
    "print(f\"LARGEST: {seqs[0]:,} bp\")\n",
    "\n",
    "# Save to Drive\n",
    "!cp -r assembly {BASE_DIR}/intermediate/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: ORF Prediction (Prodigal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Prodigal\n",
    "!apt-get install -qq prodigal\n",
    "\n",
    "print(\"Running Prodigal ORF prediction...\")\n",
    "print(\"Parameters:\")\n",
    "print(\"  -p meta: Metagenomic mode (required for mixed organisms)\")\n",
    "print()\n",
    "\n",
    "# Run Prodigal in metagenomic mode\n",
    "cmd = \"\"\"prodigal \\\n",
    "    -i assembly/final.contigs.fa \\\n",
    "    -a proteins.faa \\\n",
    "    -d genes.fna \\\n",
    "    -o genes.gff \\\n",
    "    -f gff \\\n",
    "    -p meta \\\n",
    "    -q\"\"\"\n",
    "\n",
    "!{cmd}\n",
    "log_command(cmd)\n",
    "\n",
    "# Count ORFs\n",
    "total_orfs = !grep -c \">\" proteins.faa\n",
    "total_orfs = int(total_orfs[0])\n",
    "\n",
    "# Count complete vs partial\n",
    "complete = !grep -c \"partial=00\" proteins.faa\n",
    "complete = int(complete[0])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 4 COMPLETE: ORF Prediction\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ“Š TRANSITION LOG:\")\n",
    "print(f\"INPUT:  {len(seqs):,} contigs\")\n",
    "print(f\"OUTPUT: {total_orfs:,} proteins predicted\")\n",
    "print(f\"COMPLETE ORFs: {complete:,}\")\n",
    "print(f\"PARTIAL ORFs: {total_orfs - complete:,}\")\n",
    "\n",
    "# Save to Drive\n",
    "!mkdir -p {BASE_DIR}/intermediate/orfs\n",
    "!cp proteins.faa genes.fna genes.gff {BASE_DIR}/intermediate/orfs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5A: Build UniRef90 Database (Using Your File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install DIAMOND\n",
    "!wget -q https://github.com/bbuchfink/diamond/releases/download/v2.1.8/diamond-linux64.tar.gz\n",
    "!tar -xzf diamond-linux64.tar.gz\n",
    "!mv diamond /usr/local/bin/\n",
    "\n",
    "# Check if database already exists\n",
    "UNIREF90_DB = f'{BASE_DIR}/uniref90.dmnd'\n",
    "\n",
    "if os.path.exists(UNIREF90_DB):\n",
    "    print(f\"âœ… Found existing UniRef90 database!\")\n",
    "    print(f\"   Copying to working directory...\")\n",
    "    !cp \"{UNIREF90_DB}\" uniref90.dmnd\n",
    "else:\n",
    "    print(\"Building UniRef90 database from your uniref90.fasta.gz...\")\n",
    "    print(\"This will take ~15-20 minutes.\")\n",
    "    print()\n",
    "    !diamond makedb --in \"{UNIREF90_FASTA}\" -d uniref90 --threads {THREADS}\n",
    "    # Save for future use\n",
    "    !cp uniref90.dmnd \"{BASE_DIR}/\"\n",
    "    print(f\"\\nâœ… Database saved to {BASE_DIR}/uniref90.dmnd\")\n",
    "\n",
    "!ls -lh uniref90.dmnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5B: DIAMOND Search vs UniRef90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running DIAMOND search against UniRef90...\")\n",
    "print(\"Parameters:\")\n",
    "print(\"  --id 25: Minimum 25% identity (twilight zone threshold)\")\n",
    "print(\"  --evalue 1e-5: Statistical significance cutoff\")\n",
    "print(\"  --sensitive: More thorough search\")\n",
    "print()\n",
    "print(\"This will take 2-3 hours...\")\n",
    "\n",
    "cmd = \"\"\"diamond blastp \\\n",
    "    -q proteins.faa \\\n",
    "    -d uniref90 \\\n",
    "    -o uniref90_hits.m8 \\\n",
    "    --id 25 \\\n",
    "    --evalue 1e-5 \\\n",
    "    --sensitive \\\n",
    "    --threads 4 \\\n",
    "    --max-target-seqs 1\"\"\"\n",
    "\n",
    "!{cmd}\n",
    "log_command(cmd)\n",
    "\n",
    "uniref_hits = !cut -f1 uniref90_hits.m8 | sort -u | wc -l\n",
    "uniref_hits = int(uniref_hits[0])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 5B COMPLETE: UniRef90 Search\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ“Š TRANSITION LOG:\")\n",
    "print(f\"INPUT:  {total_orfs:,} proteins\")\n",
    "print(f\"OUTPUT: {uniref_hits:,} proteins with UniRef90 hits\")\n",
    "print(f\"NO HITS: {total_orfs - uniref_hits:,} proteins (potential novel)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5C: SwissProt Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and build SwissProt database\n",
    "print(\"Downloading SwissProt database (~90 MB)...\")\n",
    "!wget -q https://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/swissprot.gz\n",
    "!gunzip -f swissprot.gz\n",
    "!diamond makedb --in swissprot -d swissprot --quiet\n",
    "\n",
    "# Search\n",
    "print(\"Searching against SwissProt...\")\n",
    "cmd = \"\"\"diamond blastp \\\n",
    "    -q proteins.faa \\\n",
    "    -d swissprot \\\n",
    "    -o swissprot_hits.m8 \\\n",
    "    --id 25 \\\n",
    "    --evalue 1e-5 \\\n",
    "    --sensitive \\\n",
    "    --threads 4\"\"\"\n",
    "\n",
    "!{cmd}\n",
    "log_command(cmd)\n",
    "\n",
    "swiss_hits = !cut -f1 swissprot_hits.m8 | sort -u | wc -l\n",
    "swiss_hits = int(swiss_hits[0])\n",
    "\n",
    "print(f\"\\nðŸ“Š SwissProt hits: {swiss_hits:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5D: Human Contamination Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download human proteome\n",
    "print(\"Downloading human proteome (~15 MB)...\")\n",
    "!wget -q https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/reference_proteomes/Eukaryota/UP000005640/UP000005640_9606.fasta.gz\n",
    "!gunzip -f UP000005640_9606.fasta.gz\n",
    "!diamond makedb --in UP000005640_9606.fasta -d human --quiet\n",
    "\n",
    "# Search with relaxed thresholds\n",
    "print(\"Checking for human contamination...\")\n",
    "cmd = \"\"\"diamond blastp \\\n",
    "    -q proteins.faa \\\n",
    "    -d human \\\n",
    "    -o human_hits.m8 \\\n",
    "    --id 50 \\\n",
    "    --evalue 1e-10 \\\n",
    "    --threads 4\"\"\"\n",
    "\n",
    "!{cmd}\n",
    "log_command(cmd)\n",
    "\n",
    "human_hits_count = !cut -f1 human_hits.m8 | sort -u | wc -l\n",
    "human_hits_count = int(human_hits_count[0])\n",
    "\n",
    "print(f\"\\nðŸ“Š Human contamination: {human_hits_count} proteins\")\n",
    "if human_hits_count == 0:\n",
    "    print(\"âœ… PASS - No human contamination detected\")\n",
    "else:\n",
    "    print(\"âš ï¸ WARNING - Human contamination detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5E: Chimera Detection (VSEARCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install VSEARCH\n",
    "!wget -q https://github.com/torognes/vsearch/releases/download/v2.22.1/vsearch-2.22.1-linux-x86_64.tar.gz\n",
    "!tar -xzf vsearch-2.22.1-linux-x86_64.tar.gz\n",
    "!cp vsearch-2.22.1-linux-x86_64/bin/vsearch /usr/local/bin/\n",
    "\n",
    "print(\"Running chimera detection...\")\n",
    "cmd = \"\"\"vsearch \\\n",
    "    --uchime_denovo genes.fna \\\n",
    "    --chimeras chimeras.fna \\\n",
    "    --nonchimeras clean_genes.fna \\\n",
    "    --quiet\"\"\"\n",
    "\n",
    "!{cmd}\n",
    "log_command(cmd)\n",
    "\n",
    "chimera_count = !grep -c \">\" chimeras.fna 2>/dev/null || echo \"0\"\n",
    "chimera_count = int(chimera_count[0])\n",
    "\n",
    "print(f\"\\nðŸ“Š Potential chimeras detected: {chimera_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Extract Novel Proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all protein IDs\n",
    "all_proteins = set()\n",
    "with open('proteins.faa') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            all_proteins.add(line[1:].split()[0])\n",
    "\n",
    "# Get proteins WITH hits\n",
    "def get_hits(filename):\n",
    "    hits = set()\n",
    "    try:\n",
    "        with open(filename) as f:\n",
    "            for line in f:\n",
    "                hits.add(line.split()[0])\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    return hits\n",
    "\n",
    "uniref90_hits = get_hits('uniref90_hits.m8')\n",
    "swissprot_hits = get_hits('swissprot_hits.m8')\n",
    "human_hits = get_hits('human_hits.m8')\n",
    "\n",
    "# Novel = no hits anywhere\n",
    "all_hits = uniref90_hits | swissprot_hits | human_hits\n",
    "novel = all_proteins - all_hits\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 6: EXTRACT NOVEL PROTEINS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ“Š TRANSITION LOG:\")\n",
    "print(f\"Total proteins: {len(all_proteins):,}\")\n",
    "print(f\"UniRef90 hits: {len(uniref90_hits):,}\")\n",
    "print(f\"SwissProt hits: {len(swissprot_hits):,}\")\n",
    "print(f\"Human hits: {len(human_hits):,}\")\n",
    "print(f\"\\nðŸ”¬ NOVEL (no hits in any database): {len(novel):,}\")\n",
    "\n",
    "# Extract novel sequences\n",
    "seqs = {}\n",
    "with open('proteins.faa') as f:\n",
    "    h = None\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            h = line[1:].split()[0]\n",
    "            seqs[h] = line\n",
    "        else:\n",
    "            seqs[h] += line\n",
    "\n",
    "with open('novel_raw.faa', 'w') as f:\n",
    "    for pid in novel:\n",
    "        f.write(seqs[pid])\n",
    "\n",
    "log_command(f\"Extracted {len(novel)} novel proteins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Quality Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load novel proteins\n",
    "novels = []\n",
    "with open('novel_raw.faa') as f:\n",
    "    h, s = None, ''\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            if h: novels.append((h, s))\n",
    "            h, s = line[1:].strip(), ''\n",
    "        else:\n",
    "            s += line.strip()\n",
    "    if h: novels.append((h, s))\n",
    "\n",
    "# Apply filters\n",
    "filtered = []\n",
    "rejected = {'short': 0, 'no_start': 0, 'internal_stop': 0, 'partial': 0}\n",
    "\n",
    "for header, seq in novels:\n",
    "    if len(seq) < 100:\n",
    "        rejected['short'] += 1\n",
    "    elif not seq.startswith('M'):\n",
    "        rejected['no_start'] += 1\n",
    "    elif '*' in seq[:-1]:\n",
    "        rejected['internal_stop'] += 1\n",
    "    elif 'partial=10' in header or 'partial=01' in header or 'partial=11' in header:\n",
    "        rejected['partial'] += 1\n",
    "    else:\n",
    "        filtered.append((header, seq))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 7: QUALITY FILTERING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ“Š TRANSITION LOG:\")\n",
    "print(f\"INPUT:  {len(novels):,} novel proteins\")\n",
    "print(f\"OUTPUT: {len(filtered):,} quality-filtered proteins\")\n",
    "print(f\"\\nREJECTED:\")\n",
    "for reason, count in sorted(rejected.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {reason}: {count:,}\")\n",
    "\n",
    "# Save filtered\n",
    "with open(f'{BASE_DIR}/output/truly_novel.faa', 'w') as f:\n",
    "    for h, s in filtered:\n",
    "        f.write(f'>{h}\\n{s}\\n')\n",
    "\n",
    "log_command(f\"Quality filtered: {len(novels)} -> {len(filtered)}\")\n",
    "print(f\"\\nâœ… Saved to: {BASE_DIR}/output/truly_novel.faa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8A: Amino Acid Composition Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "EXPECTED = {'A': 0.083, 'R': 0.055, 'N': 0.041, 'D': 0.055, 'C': 0.014,\n",
    "            'Q': 0.039, 'E': 0.068, 'G': 0.071, 'H': 0.023, 'I': 0.060,\n",
    "            'L': 0.097, 'K': 0.058, 'M': 0.024, 'F': 0.039, 'P': 0.047,\n",
    "            'S': 0.066, 'T': 0.053, 'W': 0.011, 'Y': 0.029, 'V': 0.069}\n",
    "\n",
    "all_aa = ''.join(s for h, s in filtered)\n",
    "counts = Counter(all_aa)\n",
    "total = sum(counts[aa] for aa in EXPECTED.keys() if aa in counts)\n",
    "\n",
    "chi_sq = 0\n",
    "for aa, exp in EXPECTED.items():\n",
    "    obs = counts.get(aa, 0) / total if total > 0 else 0\n",
    "    chi_sq += ((obs - exp) ** 2) / exp\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VERIFICATION: Amino Acid Composition\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Chi-squared: {chi_sq:.4f}\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  < 0.3 = Normal protein composition\")\n",
    "print(\"  > 0.5 = Suspicious (possible artifacts)\")\n",
    "verdict_aa = \"PASS\" if chi_sq < 0.3 else \"FAIL\"\n",
    "print(f\"\\nðŸ“Š VERDICT: {verdict_aa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8B: Protein Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(seq):\n",
    "    issues = []\n",
    "    for aa in 'AGLPQRS':\n",
    "        if aa * 5 in seq: issues.append('repetitive')\n",
    "    if sum(1 for a in seq if a in 'AILMFWV') / len(seq) > 0.6:\n",
    "        issues.append('too_hydrophobic')\n",
    "    if sum(1 for a in seq if a in 'DEKR') / len(seq) > 0.4:\n",
    "        issues.append('too_charged')\n",
    "    return 'likely_real' if len(issues) == 0 else 'possibly_artifact'\n",
    "\n",
    "real, artifact = 0, 0\n",
    "for h, s in filtered:\n",
    "    if classify(s) == 'likely_real':\n",
    "        real += 1\n",
    "    else:\n",
    "        artifact += 1\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VERIFICATION: Protein Classification\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Likely real: {real} ({real/len(filtered)*100:.1f}%)\")\n",
    "print(f\"Possibly artifact: {artifact}\")\n",
    "verdict_class = \"PASS\" if real/len(filtered) > 0.9 else \"REVIEW\"\n",
    "print(f\"\\nðŸ“Š VERDICT: {verdict_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8C: Genomic Context Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Parse GFF to get ORF positions\n",
    "contig_orfs = defaultdict(list)\n",
    "with open('genes.gff') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('#') or not line.strip():\n",
    "            continue\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) >= 9 and parts[2] == 'CDS':\n",
    "            contig = parts[0]\n",
    "            start, end = int(parts[3]), int(parts[4])\n",
    "            pid_match = re.search(r'ID=([^;]+)', parts[8])\n",
    "            if pid_match:\n",
    "                pid = pid_match.group(1)\n",
    "                contig_orfs[contig].append((start, end, pid))\n",
    "\n",
    "# Find isolated ORFs\n",
    "novel_ids = set(h.split()[0] for h, s in filtered)\n",
    "isolated_novels = []\n",
    "\n",
    "for contig, orfs in contig_orfs.items():\n",
    "    for start, end, pid in orfs:\n",
    "        if pid in novel_ids:\n",
    "            if len(orfs) == 1:\n",
    "                isolated_novels.append((pid, 'only_orf_on_contig'))\n",
    "            elif max(e for s, e, p in orfs) - min(s for s, e, p in orfs) < 500:\n",
    "                isolated_novels.append((pid, 'very_short_contig'))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VERIFICATION: Genomic Context\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Isolated ORFs (lower confidence): {len(isolated_novels)}\")\n",
    "print(f\"Contextual ORFs (higher confidence): {len(novel_ids) - len(isolated_novels)}\")\n",
    "verdict_context = \"PASS\" if len(isolated_novels) < len(novel_ids) * 0.3 else \"REVIEW\"\n",
    "print(f\"\\nðŸ“Š VERDICT: {verdict_context}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8D: Generate Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "classifications = {\n",
    "    'type_a_completely_novel': [],\n",
    "    'type_b_structure_known': [],\n",
    "    'type_c_remote_homolog': [],\n",
    "    'type_d_domain_hybrid': [],\n",
    "    'type_e_artifact': []\n",
    "}\n",
    "\n",
    "# Add artifacts\n",
    "artifact_ids = set()\n",
    "for pid, reason in isolated_novels:\n",
    "    classifications['type_e_artifact'].append((pid, reason))\n",
    "    artifact_ids.add(pid)\n",
    "\n",
    "# Remaining as Type A (preliminary)\n",
    "for h, s in filtered:\n",
    "    pid = h.split()[0]\n",
    "    if pid not in artifact_ids:\n",
    "        classifications['type_a_completely_novel'].append(pid)\n",
    "\n",
    "# Generate report\n",
    "report = f\"\"\"# Verification Report - Silent Hunter v6.0\n",
    "\n",
    "Generated: {datetime.datetime.now().isoformat()}\n",
    "\n",
    "## FAHDYCONX Transition Summary\n",
    "\n",
    "| Step | Input | Output | Change |\n",
    "|------|-------|--------|--------|\n",
    "| Raw Data | SRR6356483 | FASTQ | Converted |\n",
    "| QC | {before['total_reads']:,} reads | {after['total_reads']:,} reads | {before['total_reads']-after['total_reads']:,} removed |\n",
    "| Assembly | Clean reads | {len(seqs):,} contigs | N50={n50:,}bp |\n",
    "| ORF Prediction | Contigs | {total_orfs:,} proteins | - |\n",
    "| UniRef90 | {total_orfs:,} proteins | {len(uniref90_hits):,} hits | {total_orfs-len(uniref90_hits):,} no match |\n",
    "| Quality Filter | {len(novels):,} novel | {len(filtered):,} filtered | {len(novels)-len(filtered):,} rejected |\n",
    "\n",
    "## Final Counts\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Total proteins predicted | {len(all_proteins):,} |\n",
    "| Novel (no database hits) | {len(novel):,} |\n",
    "| Quality filtered | {len(filtered):,} |\n",
    "\n",
    "## Verification Tests\n",
    "\n",
    "| Test | Result | Verdict |\n",
    "|------|--------|--------|\n",
    "| Human contamination | {human_hits_count} matches | {\"PASS\" if human_hits_count == 0 else \"FAIL\"} |\n",
    "| AA composition | Chi-sq = {chi_sq:.3f} | {verdict_aa} |\n",
    "| Protein classification | {real/len(filtered)*100:.1f}% real | {verdict_class} |\n",
    "| Genomic context | {len(isolated_novels)} isolated | {verdict_context} |\n",
    "\n",
    "## Novel Protein Classification (Preliminary)\n",
    "\n",
    "| Type | Description | Count |\n",
    "|------|-------------|-------|\n",
    "| Type A | Completely novel | {len(classifications['type_a_completely_novel'])} |\n",
    "| Type B | Structure-known | TBD (run Foldseek) |\n",
    "| Type C | Remote homolog | TBD (run HHblits) |\n",
    "| Type D | Domain hybrid | TBD (run Pfam) |\n",
    "| Type E | Possible artifact | {len(classifications['type_e_artifact'])} |\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "**{len(filtered):,} novel protein candidates** identified from ISS metagenome.\n",
    "\"\"\"\n",
    "\n",
    "with open(f'{BASE_DIR}/output/VERIFICATION_REPORT.md', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(report)\n",
    "print(f\"\\nâœ… Report saved to: {BASE_DIR}/output/VERIFICATION_REPORT.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all intermediate files to Drive\n",
    "!mkdir -p {BASE_DIR}/intermediate/diamond\n",
    "!cp *_hits.m8 {BASE_DIR}/intermediate/diamond/ 2>/dev/null || true\n",
    "\n",
    "# Calculate final checksums\n",
    "!md5sum {BASE_DIR}/output/truly_novel.faa >> {BASE_DIR}/audit/checksums.md5\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAll files saved to: {BASE_DIR}\")\n",
    "print(\"\\nFinal output:\")\n",
    "!ls -lh {BASE_DIR}/output/\n",
    "\n",
    "print(\"\\nðŸ“‹ Next Steps:\")\n",
    "print(\"1. Review VERIFICATION_REPORT.md\")\n",
    "print(\"2. Run manual HHblits verification (20 samples)\")\n",
    "print(\"3. Run manual Foldseek verification (10 samples)\")\n",
    "print(\"4. Complete NCBI nr spot check\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
